{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CADL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdvardFoss/02456-deep-learning-with-PyTorch/blob/master/CADL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqe_xggulODz",
        "colab_type": "text"
      },
      "source": [
        "#Forcasting Collision Risk for ESA Satellites by training a RNN on real-world CDMs\n",
        "\n",
        "###02456 Deep Learning Project (Fall 2019)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QbM_9pypqgh",
        "colab_type": "text"
      },
      "source": [
        "By Edvard Foss (s191652)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2342USypG6G",
        "colab_type": "text"
      },
      "source": [
        "##Acronyms\n",
        "\n",
        "*   CDM: Conjunction Data Message\n",
        "*   ESA: European Space Agency\n",
        "*   LSTM: Long Short-Team Memory\n",
        "*   RNN: Recurrent Neural Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY5YazpPlXGW",
        "colab_type": "text"
      },
      "source": [
        "##Motivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNSrBsQSlcdR",
        "colab_type": "text"
      },
      "source": [
        "The Space Debris Crisis caused by the Kessler Syndrom is getting worse exponentially. As a space entusiast I would like to take part in developing ways of mitigating the effect and avoid furter buildup of debris in space. Avoiding orbital collisions would have a significant effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWWp9D5RmF0F",
        "colab_type": "text"
      },
      "source": [
        "##Background\n",
        "\n",
        "Objects in orbit larger than 10cm are tracked by the Space Surveillance Network and their position is released in the form of a globally shared catalogue.\n",
        "\n",
        "In the context of this activity, the orbits of these satellites are propagated and when a close approach with any object in the catalogue is detected a Conjunction Data Message (CDM) is assembled and released. Each CDM contains multiple attributes about the approach.\n",
        "\n",
        "For this project, due to Space Debris Office requirements, we are going to create a RNN that will, by inputting the the CDMs recorded up to 2 days prior to the closest approach, predict the final risk of collision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUMiEgUjli76",
        "colab_type": "text"
      },
      "source": [
        "##Milestones\n",
        "\n",
        "\n",
        "1.   Pre-process data and make the data into a supervised learning problem\n",
        "2.   Implement the LSTM\n",
        "3.   Train and Test the network\n",
        "4.   Optimize the network\n",
        "\n",
        "if time: SigOpt\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NkXsLlJfXvV",
        "colab_type": "text"
      },
      "source": [
        "##Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tauyKXZQfvp4",
        "colab_type": "text"
      },
      "source": [
        "###Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtAlRgcMoCpy",
        "colab_type": "code",
        "outputId": "33709325-5bf3-4f10-d5e8-a97e3cdefa86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8C2u4ARqzb6",
        "colab_type": "text"
      },
      "source": [
        "For this project we will be a regression problem where we would like to predict the `max_risk_estimate` for the last possible CDM from all data up until `time_to_tca`=2. We will therefor remove all the data we are not going to use. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqV4QeYpf1a9",
        "colab_type": "text"
      },
      "source": [
        "##Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vzuXIkln6Fr",
        "colab_type": "code",
        "outputId": "57212872-b8c8-431e-800d-e7a41cf30022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load\n",
        "from pandas import read_csv, Grouper\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "# load dataset\n",
        "data = read_csv('/content/drive/My Drive/DL/train_data.csv', usecols=[0,1,4]) #max 4\n",
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(162634, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXkKIOAmf5BV",
        "colab_type": "text"
      },
      "source": [
        "##Split into time-series sequences & prepare for RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWx4V9bTx1Z_",
        "colab_type": "text"
      },
      "source": [
        "Now we will splitt into the sequences. By group and only use samples that include data for more than 2 days prior to the TCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waPx3h1nqlad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "group_sequences=data.groupby(['event_id'])\n",
        "\n",
        "#delete all events that have less than 2 days of data\n",
        "events=group_sequences.first()\n",
        "events2=group_sequences.last()\n",
        "\n",
        "threshold=int(2)#gives series of T/F if the sample/group starts its TCA LESS THAN 2 days before\n",
        "events_tca_correct= events['time_to_tca'].apply(lambda x: x < threshold)\n",
        "events2_tca_correct=events2['time_to_tca'].apply(lambda y: y > threshold)\n",
        "\n",
        "\n",
        "#Groups we DO NOT WANT to keep\n",
        "index_for_data_groups=list(events_tca_correct[events_tca_correct].index)\n",
        "index_for_data_groups2=list(events2_tca_correct[events2_tca_correct].index)\n",
        "indi=index_for_data_groups+index_for_data_groups2\n",
        "\n",
        "#Remove all bad data\n",
        "indi=np.sort(indi)\n",
        "\n",
        "index_del=[]\n",
        "#Get all indexes\n",
        "for t in indi:\n",
        "    index=list(group_sequences.get_group(t).index)\n",
        "    index_del=index_del+index\n",
        "\n",
        "data_new=data.drop(index_del)\n",
        "\n",
        "#split into samples by event_id\n",
        "\n",
        "samples=[]\n",
        "#get index to what we want\n",
        "group_seq=data_new.groupby(['event_id'])\n",
        "group_samples=list(group_seq.groups.keys())\n",
        "\n",
        "for i in group_samples:\n",
        "    df_sample=group_seq.get_group(i)\n",
        "    df_risk = df_sample['max_risk_estimate']\n",
        "    sample= df_risk.values\n",
        "    if len(sample)>6:\n",
        "      samples.append(sample)\n",
        "#8892 samples\n",
        "samples=np.asarray(samples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiZchab_aDrH",
        "colab_type": "code",
        "outputId": "b7ccb12d-f028-4d57-f3de-c01e86b8ab98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Number of samples for training: ' +str(len(samples)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples for training: 7946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QSVR4HKhkjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(samples[40])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew1HUCNbUfPZ",
        "colab_type": "text"
      },
      "source": [
        "## Partitioning the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW2ZpdyzUYxu",
        "colab_type": "text"
      },
      "source": [
        "To build our dataset, we need to create inputs and targets for each sequences and partition sentences it into training, validation and test sets. 80%, 10% and 10% is a common distribution. \n",
        "\n",
        "We can use PyTorch's `Dataset` class to build a simple dataset where we can easily retrieve (inputs, targets) pairs for each of our sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaFrqe_7UwHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the size of the dataset\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Retrieve inputs and targets at the given index\n",
        "        X = self.inputs[index]\n",
        "        y = self.targets[index]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    \n",
        "def create_datasets(samples, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
        "    # Define partition sizes\n",
        "    num_train = int(len(samples)*p_train)\n",
        "    num_val = int(len(samples)*p_val)\n",
        "    num_test = int(len(samples)*p_test)\n",
        "\n",
        "    # Split samples into partitions\n",
        "    samples_train = samples[:num_train]\n",
        "    samples_val = samples[num_train:num_train+num_val]\n",
        "    samples_test = samples[-num_test:]\n",
        "\n",
        "    def get_inputs_targets_from_samples(samples): \n",
        "        #criterion = nn.MSELoss()\n",
        "        # Define empty lists\n",
        "        inputs, targets, no_pre, loss = [], [], [],[]\n",
        "        \n",
        "        #inputs are what is left after that the 6 last values of the sample \n",
        "        #has been extracted to be the output\n",
        "        for i in range(len(samples)):\n",
        "            len_sample=len(samples[i])\n",
        "            inputs.append(samples[i][:(len_sample-6)])\n",
        "            no_pre.append(samples[i][-6:-5])\n",
        "            targets.append(samples[i][-1:])\n",
        "            #loss.append(criterion(torch.Tensor(no_pre),torch.Tensor(targets)))\n",
        "        print(sum(loss)/len(samples))\n",
        "        print(targets[0])\n",
        "        print(no_pre[0])\n",
        "        return inputs, targets\n",
        "\n",
        "    # Get inputs and targets for each partition\n",
        "    inputs_train, targets_train = get_inputs_targets_from_samples(samples_train)\n",
        "    inputs_val, targets_val = get_inputs_targets_from_samples(samples_val)\n",
        "    inputs_test, targets_test = get_inputs_targets_from_samples(samples_test)\n",
        "\n",
        "    # Create datasets\n",
        "    training_set = dataset_class(inputs_train, targets_train)\n",
        "    validation_set = dataset_class(inputs_val, targets_val)\n",
        "    test_set = dataset_class(inputs_test, targets_test)\n",
        "\n",
        "    return training_set, validation_set, test_set\n",
        "    \n",
        "\n",
        "training_set, validation_set, test_set = create_datasets(samples, Dataset)\n",
        "\n",
        "print(f'We have {len(training_set)} samples in the training set.')\n",
        "print(f'We have {len(validation_set)} samples in the validation set.')\n",
        "print(f'We have {len(test_set)} samples in the test set.')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReDl0Oljr4rC",
        "colab_type": "text"
      },
      "source": [
        "###Zero-padding\n",
        "We need to find the maximum input length such that we can zero-pad the inputs to the correct length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_msjA9e5bPN",
        "colab_type": "code",
        "outputId": "a3a9fd4f-c0bc-4f7c-bc6c-e37c72f242f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "length=1\n",
        "\n",
        "for i in range(len(training_set)):\n",
        "  new_length=len(training_set[i][0])\n",
        "  if new_length>length:\n",
        "    length=new_length\n",
        "  else:\n",
        "    length=length\n",
        "print('Length of longest input sequence: ' + str(length))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of longest input sequence: 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OQWBFJYslp1",
        "colab_type": "text"
      },
      "source": [
        "Zero-padding trail. Example of a zero-padded input sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KosHgqV99H0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch\n",
        "seq=training_set[0][0]\n",
        "max_length=17\n",
        "seq = torch.Tensor(seq)\n",
        "length=len(seq)\n",
        "dif=max_length-length\n",
        "seq=F.pad(seq, pad=(0,dif),mode='constant', value=0)\n",
        "\n",
        "print(seq)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAUa4iLovQGG",
        "colab_type": "text"
      },
      "source": [
        "Test: List of all sample lengths. Must be done with all inputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfTIM4G6mp9c",
        "colab_type": "text"
      },
      "source": [
        "#Introduction to LSTM\n",
        "\n",
        "A vanilla RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the Long Short-Term Memory (LSTM) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. In this exercise, we will focus on LSTM but you would easily be able to go ahead and implement the GRU as well based on the principles that you learn here.\n",
        "\n",
        "LSTMs have also shown good results for sequences of unequal timesteps.\n",
        "\n",
        "Below is a figure of the LSTM cell:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwNkd1j_mwlt",
        "colab_type": "text"
      },
      "source": [
        "![lstm](https://i.imgur.com/3VkmUCe.png)\n",
        "Source: https://arxiv.org/abs/1412.7828"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy4P3zbbm02K",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The LSTM cell contains three gates, input, forget, output gates and a memory cell.\n",
        "The output of the LSTM unit is computed with the following functions, where $\\sigma = \\mathrm{softmax}$.\n",
        "We have input gate $i$, forget gate $f$, and output gate $o$ defines as\n",
        "\n",
        "- $i = \\sigma ( W^i [h_{t-1}, x_t])$\n",
        "\n",
        "- $f = \\sigma ( W^f [h_{t-1},x_t])$\n",
        "\n",
        "- $o = \\sigma ( W^o [h_{t-1},x_t])$\n",
        "\n",
        "where $W^i, W^f, W^o$ are weight matrices applied to a concatenated $h_{t-1}$ (hidden state vector) and $x_t$ (input vector)  for each respective gate.\n",
        "\n",
        "$h_{t-1}$, from the previous time step along with the current input $x_t$ are used to compute the a candidate $g$\n",
        "\n",
        "- $g = \\mathrm{tanh}( W^g [h_{t-1}, x_t])$\n",
        "\n",
        "The value of the cell's memory, $c_t$, is updated as\n",
        "\n",
        "- $c_t = c_{t-1} \\circ f + g \\circ i$\n",
        "\n",
        "where $c_{t-1}$ is the previous memory, and $\\circ$ refers to element-wise multiplication.\n",
        "\n",
        "The output, $h_t$, is computed as\n",
        "\n",
        "- $h_t = \\mathrm{tanh}(c_t) \\circ o$\n",
        "\n",
        "and it is used for both the timestep's output and the next timestep, whereas $c_t$ is exclusively sent to the next timestep.\n",
        "This makes $c_t$ a memory feature, and is not used directly to compute the output of the timestep."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OQtngYzXBwv",
        "colab_type": "text"
      },
      "source": [
        "#Implementing LSTM\n",
        "\n",
        "The collision is a deterministic event and as such its probability after the TCA is either 1 or 0. The risk reported in the CDMs, and predicted here, is some kind of measure of the confidence we have on a collision happening. The number is used to decide on whether to perform an avoidance manouvre or not (note that such a decision may be wrong after all as the collision would never have happened anyway,there is basically no way to know). In this project we wish to create a model allowing to make that decision in advance, i.e. based only on the CDMs with TCA > 2 days. As a ground truth to know whether the Space Debris Office at ESA would decide to perform the manouvre or not, they use the take the risk of the latest available CDMs.\n",
        "\n",
        "A lot of human factors and non-mathematical processes enter in this a set-up. This problem is therefor very suitable for a neural network. As the samples are sequences a RNN should be used. However,the vanilla RNN suffers from [the vanishing gradients problem as stated earlier. For this project we will implement our RNN model as a LSTM.\n",
        "\n",
        "We are going to mask the inputs using the pad pack utility. The max_length will be put to 17 as that is the longest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RftsliGQgXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, seq_len, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Hidden dimensions\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Sequence length\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # Number of hidden layers\n",
        "        self.layer_dim = layer_dim\n",
        "\n",
        "        # Building your LSTM\n",
        "        # batch_first=True causes input/output tensors to be of shape\n",
        "        # (batch_dim, seq_dim, feature_dim)\n",
        "        self.lstm = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, dropout= 0.2)\n",
        "        #dropout had a worsening effect\n",
        "\n",
        "        # Readout layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        # Initiate hidden layer\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Before we've done anything, we dont have any hidden state.\n",
        "        # Refer to the Pytorch documentation to see exactly\n",
        "        # why they have this dimensionality.\n",
        "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
        "        return (torch.zeros(1, 1, self.hidden_dim),\n",
        "                torch.zeros(1, 1, self.hidden_dim))\n",
        "\n",
        "        \n",
        "    def forward(self, x, length_seq):\n",
        "      #  print(x.size())\n",
        "        \n",
        "       pack = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=(torch.LongTensor([length_seq]).cpu().numpy()), batch_first=True)\n",
        "\n",
        "       #print('Before LSTM')\n",
        "       #print(pack)\n",
        "\n",
        "        # 17 time steps\n",
        "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
        "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
        "       packed, (hn, cn) = self.lstm(x)\n",
        "\n",
        "      # print('After LSTM')\n",
        "       #print(packed)\n",
        "       #print(packed.size())\n",
        "\n",
        "       #out=torch.nn.utils.rnn.pad_packed_sequence(packed, total_length=17, batch_first=True)\n",
        "       #print(out[0])\n",
        "       # print('After unpack')\n",
        "       # print(out)\n",
        "       #out=out.reshape(1,17,1)        \n",
        "        # Index hidden state of last time step\n",
        "       out = self.fc(packed[:, -1, :]) \n",
        "       return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKztxm_rfPGS",
        "colab_type": "code",
        "outputId": "3fecef34-85f2-4b4a-b883-c9efeef4b01f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "pip install adabound"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting adabound\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/44/0c2c414effb3d9750d780b230dbb67ea48ddc5d9a6d7a9b7e6fcc6bdcff9/adabound-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from adabound) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.0->adabound) (1.17.4)\n",
            "Installing collected packages: adabound\n",
            "Successfully installed adabound-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj3SDPMyYGLf",
        "colab_type": "text"
      },
      "source": [
        "##Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EZUCFX24bcpq",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import adabound\n",
        "\n",
        "# Model paramaters\n",
        "\n",
        "output_dim = 1\n",
        "input_dim =1\n",
        "seq_len=17\n",
        "max_length=17\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 35\n",
        "hidden_dim = 100 # 100. worse with 150\n",
        "layer_dim = 2\n",
        "l_r=0.001 #ADAM: Above 0.001, increasing the learning rate increased the time to train and also increased the variance in training time (as compared to a linear function of model size).\n",
        "\n",
        "# Create network\n",
        "net = Net(seq_len, input_dim, hidden_dim, layer_dim, output_dim)\n",
        "\n",
        "\n",
        "# Define a loss function and optimizer for this problem\n",
        "criterion = nn.MSELoss() #https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c 1)MSE\n",
        "optimizer = adabound.AdaBound(net.parameters(), lr=0.0001, final_lr=0.001) #1) ADAM torch.optim.Adam(net.parameters(), lr=l_r)    # 2) \n",
        "\n",
        "# Track loss\n",
        "training_loss, validation_loss = [], []\n",
        "\n",
        "# Track accuracy\n",
        "accuracy = []\n",
        "\n",
        "# Track correct class\n",
        "correct=int(0)\n",
        "\n",
        "# For each epoch\n",
        "for i in range(num_epochs):\n",
        "    \n",
        "    # Track loss\n",
        "    epoch_training_loss = 0\n",
        "    epoch_validation_loss = 0\n",
        "    \n",
        "    net.eval()\n",
        "        \n",
        "    # For each sample in validation set\n",
        "    for inputs, targets in validation_set:\n",
        "                \n",
        "        # Convert input to tensor & zero-pad\n",
        "        inputs = torch.FloatTensor(inputs)\n",
        "        length=len(inputs)\n",
        "        dif=max_length-length\n",
        "        inputs=F.pad(inputs, pad=(0,dif),mode='constant', value=0)\n",
        "        inputs=inputs.reshape(1,17,1)\n",
        "        \n",
        "        # Convert target to tensor\n",
        "        targets = torch.FloatTensor(targets)       \n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = net.forward(inputs, length)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        #Compute accuracy\n",
        "        accuracy.append(abs(outputs.item()-targets.item())/abs(targets.item()))\n",
        "\n",
        "              \n",
        "        # Update loss\n",
        "        epoch_validation_loss += loss.detach().numpy()\n",
        "    \n",
        "    net.train()\n",
        "    \n",
        "    # For each sentence in training set\n",
        "    for inputs, targets in training_set:\n",
        "             \n",
        "         # Convert input to tensor & zero-pad\n",
        "        inputs = torch.FloatTensor(inputs)\n",
        "        length=len(inputs)\n",
        "        dif=max_length-length\n",
        "        inputs=F.pad(inputs, pad=(0,dif),mode='constant', value=0)\n",
        "        inputs=inputs.reshape(1,17,1)\n",
        "\n",
        "        # Convert target to tensor\n",
        "        targets = torch.FloatTensor(targets)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = net(inputs, length)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update loss\n",
        "        epoch_training_loss += loss.detach().numpy()\n",
        "        \n",
        "    # Save loss for plot\n",
        "    training_loss.append(epoch_training_loss/len(training_set))\n",
        "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
        "\n",
        "    # Print loss every 5 epochs\n",
        "    if i % 5 == 0:\n",
        "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
        "\n",
        "        \n",
        "# Get first sample in test set\n",
        "inputs, targets = test_set[0]\n",
        "\n",
        "# Convert input to tensor\n",
        "inputs = torch.FloatTensor(inputs)\n",
        "length=len(inputs)\n",
        "dif=max_length-length\n",
        "inputs=F.pad(inputs, pad=(0,dif),mode='constant', value=0)\n",
        "inputs=inputs.reshape(1,17,1)\n",
        "# Convert target to tensor\n",
        "targets = torch.Tensor(targets)\n",
        "\n",
        "# Forward pass\n",
        "outputs = net.forward(inputs, length)\n",
        "\n",
        "print('\\nInput sequence:')\n",
        "print(inputs)\n",
        "\n",
        "print('\\nTarget sequence:')\n",
        "print(targets)\n",
        "\n",
        "print('\\nPredicted sequence:')\n",
        "print(outputs)\n",
        "\n",
        "print('\\nAccuracy:')\n",
        "len_acc=len(accuracy)\n",
        "sum_acc=sum(accuracy)\n",
        "acc_tot=int((1-sum_acc/len_acc)*100)\n",
        "print(str(acc_tot) + ' %')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVRjwRXDlYv3",
        "colab_type": "text"
      },
      "source": [
        "1) AdaBound shows an improved accuracy of the model. 89% accuracy and MSE=0.38 (hYPER: 15epochs, 100 hidden dim lr: 0.0001) vs. Adam 88% MSE=0.39 (hYPER: 35epochs, 100 hidden dim lr: 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pZJefPEJZqF",
        "colab_type": "text"
      },
      "source": [
        "###Plot MSE Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3DMkPpFxlm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
        "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Zj1v1QesTW",
        "colab_type": "code",
        "outputId": "e4d68e6b-14f4-4d38-fddc-484b68fb197d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_tl=training_loss[1:]\n",
        "plot_vl=validation_loss[1:]\n",
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(plot_tl))\n",
        "plt.figure()\n",
        "plt.plot(epoch, plot_tl, 'r', label='Training loss',)\n",
        "plt.plot(epoch, plot_vl, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAhGRKKcgEBGJcv8ffWR6196EKFYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_WxBOTyqcat",
        "colab_type": "text"
      },
      "source": [
        "#Testing & Classification Performance Results\n",
        "\n",
        "We run through the whole test-set to statistically evaluate the results from the predictions of the final risk. Additionally we test the model on classifying the high and low risks as explained in the paper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2vdfnWh0Qqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tar=[]\n",
        "pre=[]\n",
        "esa=[]\n",
        "\n",
        "\n",
        "for inputs, targets in test_set:\n",
        "\n",
        "  # Convert input to tensor\n",
        "  esain=inputs\n",
        "  inputs = torch.FloatTensor(inputs)\n",
        "  length=len(inputs)\n",
        "  dif=max_length-length\n",
        "  inputs=F.pad(inputs, pad=(0,dif),mode='constant', value=0)\n",
        "  inputs=inputs.reshape(1,17,1)\n",
        "\n",
        "  # Convert target to tensor\n",
        "  targets = torch.Tensor(targets)\n",
        "\n",
        "  # Forward pass\n",
        "  outputs = net.forward(inputs, length)\n",
        "\n",
        "  #Classify and calculate accuracy\n",
        "  esa.append(esain[-1]>=-6)\n",
        "  pre.append(outputs.item()>=-6)\n",
        "  tar.append(targets.item()>=-6)\n",
        "\n",
        "print('\\nInput Risk Sequence:')\n",
        "print(inputs)\n",
        "\n",
        "print('\\nTarget Risk:')\n",
        "print(targets)\n",
        "print('Target Risk in Percentage: ' + str(10**(targets.item()))+ ' %')\n",
        "\n",
        "print('\\nPredicted Final Risk:')\n",
        "print(outputs)\n",
        "print('Predicted Risk in Percentage: ' + str(10**(outputs.item()))+ ' %')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzpVe1-gq0ME",
        "colab_type": "text"
      },
      "source": [
        "##Confusion matrix\n",
        "Create a confusion matrix for the low and high classification of final risks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3deTKkHEJ8w4",
        "colab_type": "text"
      },
      "source": [
        "##Confusion Matrix Stacked LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMjcxz_sk4eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(tar, pre)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSQtIrdpKBIX",
        "colab_type": "text"
      },
      "source": [
        "###Confusion Matrix ESA Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sf8tLGCKI4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "confusion_matrix(tar,esa)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEdrM58XAgwG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = classes\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9-aJzl5XmQu",
        "colab_type": "text"
      },
      "source": [
        "###Standardized Confusion Matrix (ESA model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_qJRSfoBOQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(tar, esa, classes=['Low Risk', 'High Risk'], normalize=False,cmap=plt.cm.Blues)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmbx-UIpXyrK",
        "colab_type": "text"
      },
      "source": [
        "###Standardized Confusion Matrix (Stacked LSTM model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTHgU1sLX94D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_confusion_matrix(tar, pre, classes=['Low Risk', 'High Risk'], normalize=False,cmap=plt.cm.Blues)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FMFI7Vl3Qse",
        "colab_type": "text"
      },
      "source": [
        "#Masking w/ torch.nn.utils.rnn.pack_padded_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64XR1CymL0Zd",
        "colab_type": "text"
      },
      "source": [
        "###Summary of Shape Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtMC20S0K0_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# (batch_size X max_seq_len X embedding_dim) --> Sort by seqlen ---> (batch_size X max_seq_len X embedding_dim)\n",
        "# (batch_size X max_seq_len X embedding_dim) --->      Pack     ---> (batch_sum_seq_len X embedding_dim)\n",
        "# (batch_sum_seq_len X embedding_dim)        --->      LSTM     ---> (batch_sum_seq_len X hidden_dim)\n",
        "# (batch_sum_seq_len X hidden_dim)           --->    UnPack     ---> (batch_size X max_seq_len X hidden_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQaKxGIeLlyl",
        "colab_type": "text"
      },
      "source": [
        "###Masking example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIjdx_7n6tfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Get first sample in test set\n",
        "inputs, targets = test_set[7]\n",
        "\n",
        "# Convert input to tensor\n",
        "inputs = torch.FloatTensor(inputs)\n",
        "length=len(inputs)\n",
        "dif=17-length\n",
        "inputs=F.pad(inputs, pad=(0,dif),mode='constant', value=0)\n",
        "inputs=inputs.reshape(1,17,1)\n",
        "seq_in = Variable(inputs)\n",
        "seq_length = [length]\n",
        "pack = torch.nn.utils.rnn.pack_padded_sequence(seq_in, seq_length, batch_first=True)\n",
        "print (pack)\n",
        "\n",
        "unpack=torch.nn.utils.rnn.pad_packed_sequence(pack, total_length=17, batch_first=True)\n",
        "print(unpack)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}